from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.concurrency import run_in_threadpool
from typing import List
import json
import random
import uuid
from sqlalchemy.orm import Session
from .models import schemas
from .models.database import get_db, init_db, Task, SessionLocal
from .services.places_service import PlacesService
from .services.apify_service import ApifyService
from .services.nlp_service import NLPService
from .services.youtube_service import YouTubeService
from .api import trips

app = FastAPI()

app.include_router(trips.router, prefix="/api/v1", tags=["trips"])

# åˆå§‹åŒ–æœå‹™
apify_service = ApifyService()
nlp_service = NLPService()
youtube_service = YouTubeService()
places_service = PlacesService()

@app.on_event("startup")
def on_startup():
    init_db()

@app.get("/")
def read_root():
    return {"message": "Welcome to Pubo API"}

async def process_share_task(task_id: str, url: str):
    """
    éåŒæ­¥è™•ç†å‡½æ•¸ï¼šçˆ¬å–å…§å®¹ä¸¦ä½¿ç”¨ LLM è§£æ
    """
    # èƒŒæ™¯ä»»å‹™éœ€è¦ç¨ç«‹çš„ DB Session
    db = SessionLocal()
    try:
        task = db.query(Task).filter(Task.task_id == task_id).first()
        if not task:
            return

        task.status = "processing"
        db.commit()
    
        source_type = "instagram"
        print(f"TASK {task_id}: é–‹å§‹è™•ç† {url}")
        
        # 1. çˆ¬å–å…§å®¹
        scraped_data = None
        if "youtube.com" in url or "youtu.be" in url:
            source_type = "youtube"
            # ä½¿ç”¨ YouTubeService (Sync -> Threadpool)
            scraped_data = await run_in_threadpool(youtube_service.process_video, url)
        elif "threads.net" in url or "threads.com" in url:
            source_type = "threads"
            scraped_data = await run_in_threadpool(apify_service.scrape_threads_post, url)
        else:
            source_type = "instagram"
            scraped_data = await run_in_threadpool(apify_service.scrape_instagram_post, url)
            
        if scraped_data:
            print(f"ğŸ” [Backend] Final Scraped Text: {scraped_data.get('text', '')[:200]}...") # Log å‰200å­—
            
        print(f"TASK {task_id}: {source_type} çˆ¬å–å®Œæˆï¼Œçµæœ: {True if scraped_data else False}")

        if not scraped_data:
            task.status = "failed"
            task.error = "ç„¡æ³•çˆ¬å–è©²é€£çµå…§å®¹"
            db.commit()
            return

        # 2. ä½¿ç”¨ LLM è§£æåœ°é»åç¨± (Sync -> Threadpool)
        extracted_places = await run_in_threadpool(nlp_service.extract_places, scraped_data["text"])

        # 3. ä½¿ç”¨ Google Places API æœå°‹ç¢ºåˆ‡åœ°é»è³‡è¨Š (Parallel Execution)
        import asyncio
        
        async def enrich_place(p):
            place_name = p["name"]
            # ä½¿ç”¨ run_in_threadpool ç¢ºä¿åŒæ­¥çš„ search_place ä¸æœƒé˜»å¡ Event Loop
            google_place = await run_in_threadpool(places_service.search_place, place_name)
            
            place_data = {
                "place_id": f"temp_{random.randint(1000, 9999)}",
                "name": place_name,
                "address": None,
                "latitude": 0.0,
                "longitude": 0.0,
                "category": p.get("category", "å…¶ä»–"),
                "google_place_id": None
            }

            if google_place:
                # Use Google Place ID as the primary ID to ensure deduplication on client side
                real_id = google_place.get("name", "").split("/")[-1] if "/" in google_place.get("name", "") else google_place.get("id")
                
                place_data["place_id"] = real_id
                place_data["google_place_id"] = real_id
                place_data["name"] = google_place.get("displayName", {}).get("text", place_name)
                place_data["address"] = google_place.get("formattedAddress")
                
                location = google_place.get("location", {})
                place_data["latitude"] = location.get("latitude", 0.0)
                place_data["longitude"] = location.get("longitude", 0.0)
                
                place_data["rating"] = google_place.get("rating")
                place_data["user_ratings_total"] = google_place.get("userRatingCount")
                place_data["opening_hours"] = google_place.get("regularOpeningHours")
                place_data["open_now"] = google_place.get("currentOpeningHours", {}).get("openNow")
                
                if "primaryType" in google_place:
                    place_data["category"] = google_place["primaryType"].replace("_", " ").title()

            return schemas.ContentPlaceInfo(
                place=schemas.PlaceBase(**place_data),
                evidence_text=p.get("evidence_text"),
                confidence_score=p.get("confidence_score", 0.0)
            )

        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰æœå°‹
        enriched_places = await asyncio.gather(*(enrich_place(p) for p in extracted_places))

        # 4. å°è£çµæœ
        content_base = schemas.ContentBase(
            source_type=source_type,
            source_url=url,
            title=scraped_data.get("title", f"ä¾†è‡ª {source_type} çš„åˆ†äº«"),
            text=scraped_data["text"],
            author_name=scraped_data.get("author_name"),
            author_avatar_url=scraped_data.get("author_avatar_url"),
            preview_thumbnail_url=scraped_data.get("preview_thumbnail_url"),
            published_at=None
        )

        extraction_response = schemas.ExtractionResponse(
            content=content_base,
            suggested_places=enriched_places
        )
        
        # å­˜å…¥ Result (éœ€ dump ç‚º JSON ç›¸å®¹æ ¼å¼)
        # ç”±æ–¼ Pydantic model dump éœ€è¦ dict
        task.result = json.loads(extraction_response.json())
        task.status = "completed"
        db.commit()

    except Exception as e:
        print(f"Task Error: {e}")
        task.status = "failed"
        task.error = str(e)
        db.commit()
    finally:
        db.close()

@app.post("/api/v1/share", response_model=schemas.TaskResponse)
async def share_content(request: schemas.ShareRequest, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """
    æ¥æ”¶ç¤¾ç¾¤åˆ†äº«é€£çµï¼Œå•Ÿå‹•ç•°æ­¥è§£æä»»å‹™
    """
    task_id = str(uuid.uuid4())
    
    new_task = Task(
        task_id=task_id,
        status="pending",
        target_url=request.url
    )
    db.add(new_task)
    db.commit()
    db.refresh(new_task)
    
    background_tasks.add_task(process_share_task, task_id, request.url)
    
    return schemas.TaskResponse(
        task_id=new_task.task_id,
        status=schemas.TaskStatus(new_task.status),
        result=None,
        error=None
    )

@app.get("/api/v1/task/{task_id}", response_model=schemas.TaskResponse)
async def get_task_status(task_id: str, db: Session = Depends(get_db)):
    """
    æŸ¥è©¢ä»»å‹™ç‹€æ…‹èˆ‡çµæœ
    """
    task = db.query(Task).filter(Task.task_id == task_id).first()
    if not task:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°è©²ä»»å‹™")
        
    return schemas.TaskResponse(
        task_id=task.task_id,
        status=schemas.TaskStatus(task.status),
        result=task.result,
        error=task.error
    )

@app.get("/api/v1/library/contents", response_model=List[schemas.ContentResponse])
async def list_contents():
    """
    é¡¯ç¤ºæ”¶è—åº«ä¸­çš„å…§å®¹æ¨¡å¼ (é€£çµæ¨¡å¼)
    """
    # TODO: å¾è³‡æ–™åº«è®€å–
    return []

@app.get("/api/v1/library/places", response_model=List[schemas.PlaceBase])
async def list_places():
    """
    é¡¯ç¤ºæ”¶è—åº«ä¸­çš„åœ°é»æ¨¡å¼ (åœ°é»æ¨¡å¼)
    - è¦å‰‡ï¼šåœ°é»éœ€å»é‡
    """
    # TODO: å¾è³‡æ–™åº«è®€å–ä¸¦å»é‡
    return []

@app.post("/api/v1/analyze/place", response_model=schemas.AnalyzeResponse)
async def analyze_place(request: schemas.AnalyzeRequest):
    """
    ä½¿ç”¨ AI ç”Ÿæˆåœ°é»ä»‹ç´¹
    """
    description = await run_in_threadpool(nlp_service.generate_place_description, request.name, request.address)
    return schemas.AnalyzeResponse(description=description)
